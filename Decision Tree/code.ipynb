{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2567ae8f-f0f8-43fc-961c-e6b491af0718",
   "metadata": {},
   "source": [
    "# Mentor Program\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "113c805b-7c07-45a4-a9c6-d2ebc120d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b38c50-8177-4eb6-8660-31cd0b51f3ce",
   "metadata": {},
   "source": [
    "### Global Parameters:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4b03abd3-8a9d-4b4b-9fc1-3046d5571d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEAF_SIZE = 9\n",
    "MAX_DEPTH = 10\n",
    "PERCENT_REPRESENTATION = 95\n",
    "\n",
    "TEST_SET = 'Abominable_Data_HW_LABELED_TRAINING_DATA__v750_2215.csv'\n",
    "# VALIDATION_SET = 'Abominable_VALIDATION_Data_FOR_STUDENTS_v750_2215.csv'\n",
    "VALIDATION_SET = TEST_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa0ea6-a488-43aa-b376-0373634b7d37",
   "metadata": {},
   "source": [
    "## Setting up:\n",
    "---\n",
    "\n",
    "### Read the data into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "82ef0412-ef04-45e4-bff1-b69aaf86d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>TailLn</th>\n",
       "      <th>HairLn</th>\n",
       "      <th>BangLn</th>\n",
       "      <th>Reach</th>\n",
       "      <th>EarLobes</th>\n",
       "      <th>ClassName</th>\n",
       "      <th>ClassID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.9</td>\n",
       "      <td>143.6</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>146.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.3</td>\n",
       "      <td>122.9</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>125.4</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>157.6</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhuttan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.7</td>\n",
       "      <td>143.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>145.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>146.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>49.0</td>\n",
       "      <td>149.4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>152.9</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhuttan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>48.7</td>\n",
       "      <td>155.6</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>162.8</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhuttan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>71.2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>143.2</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>34.9</td>\n",
       "      <td>156.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>158.6</td>\n",
       "      <td>0</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>58.6</td>\n",
       "      <td>136.9</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age     Ht  TailLn  HairLn  BangLn  Reach  EarLobes ClassName  ClassID\n",
       "0     52.9  143.6     6.8     9.8     3.1  146.1         0     Assam       -1\n",
       "1     43.3  122.9    14.5     5.2     4.6  125.4         0     Assam       -1\n",
       "2     46.1  154.1     4.1    10.8     5.0  157.6         1   Bhuttan        1\n",
       "3     48.7  143.0    17.2     7.7     5.1  145.5         0     Assam       -1\n",
       "4     50.0  144.0    13.4     9.6     3.1  146.5         0     Assam       -1\n",
       "...    ...    ...     ...     ...     ...    ...       ...       ...      ...\n",
       "4995  49.0  149.4     9.1    10.5     6.3  152.9         1   Bhuttan        1\n",
       "4996  48.7  155.6    10.4     9.9     6.9  162.8         1   Bhuttan        1\n",
       "4997  71.2  140.0     8.1     8.7     6.2  143.2         0     Assam       -1\n",
       "4998  34.9  156.1     8.5    10.4     6.1  158.6         0     Assam       -1\n",
       "4999  58.6  136.9    12.4     4.2     6.4  140.0         1     Assam       -1\n",
       "\n",
       "[5000 rows x 9 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the labeled data from the provided CSV file.\n",
    "csv = pd.read_csv(TEST_SET); csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81206416-3069-40c5-83dc-901d49008956",
   "metadata": {},
   "source": [
    "### Implementing the Decision Tree:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2db5f-379c-40db-926b-85b48aef83c4",
   "metadata": {},
   "source": [
    "#### Attributes and Data Rounding:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5755b159-60e1-4493-a7a7-d744b3cd2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    '''\n",
    "    The Attribute class holds general information about an attribute / column.\n",
    "    \n",
    "    The class holds data about the Column-ID of the attribute in a given vector, \n",
    "    the attribute's name, the possible values the attribute can take, and finally,\n",
    "    the attribute's quantized unit.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, name, column_id):\n",
    "        \n",
    "        self.column = column_id\n",
    "        self.name = name\n",
    "        self.is_categorical = list(csv[name].unique()) == [0,1]\n",
    "        self.__define_quantization__(name)\n",
    "        self.values = self.__define_values__(name)\n",
    "        \n",
    "    def __define_quantization__(self,name):\n",
    "        ''' \n",
    "        Set a quantization unit for the attribute; this is a means of noise-reduction.\n",
    "        '''\n",
    "        global csv\n",
    "\n",
    "        # If the attribute is age data, set the quantized unit to 2 years.\n",
    "        if name.lower() == 'age':\n",
    "            self.quantized_unit = 2\n",
    "        # Height is quantized to the nearest 4cm. \n",
    "        elif name.lower() == 'ht':\n",
    "            self.quantized_unit = 4\n",
    "        # If the data is categorical (true/false), then then we don't quanztize the data    \n",
    "        elif self.is_categorical:\n",
    "            self.quantized_unit = 1\n",
    "        # All other data is quantized to the nearest 2 units. \n",
    "        else:\n",
    "            self.quantized_unit = 2\n",
    "    \n",
    "    def __define_values__(self,name):\n",
    "        '''\n",
    "        Calculate all the different values the attribute can take on. Used for threshold-finding.\n",
    "        '''\n",
    "    \n",
    "        global csv\n",
    "        min_value = self.quantized_unit * round( min(csv[name]) / self.quantized_unit )\n",
    "        max_value = self.quantized_unit * round( max(csv[name]) / self.quantized_unit )    \n",
    "        return range(min_value, max_value + self.quantized_unit, self.quantized_unit)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Attribute[{self.column}]: {self.name} / {self.quantized_unit}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f75a65a6-ee40-497d-b1d8-026acd3553a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_data(csv, attributes):\n",
    "    for attribute in attributes:\n",
    "        csv[attribute.name] = csv[attribute.name].apply( lambda datapoint: attribute.quantized_unit * round(datapoint/attribute.quantized_unit) )\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bc92f-72c8-4b4a-aeaf-409eb1ee7450",
   "metadata": {},
   "source": [
    "#### Decision Stump and Threshold Finding:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3be0ecff-6e64-4551-bbc6-074248b45b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    '''\n",
    "    A decision stump is one level in a decision tree. \n",
    "    It uses a single attribute to split data into two partitions.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attribute = attribute\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def eval(self, datapoint):\n",
    "        '''\n",
    "        Takes in a datapoint and checks against the decision stump's threshold.\n",
    "        '''\n",
    "        if self.attribute.is_categorical:\n",
    "            return datapoint[self.attribute.column] < self.threshold\n",
    "        else:\n",
    "            return datapoint[self.attribute.column] <= self.threshold\n",
    "    \n",
    "    def __repr__(self):\n",
    "        check_statement = ''\n",
    "        \n",
    "        if self.attribute.is_categorical:\n",
    "            check_statement = f'if ( {self.attribute.name.lower()} < {self.threshold} ):'\n",
    "        else:\n",
    "            check_statement = f'if ( {self.attribute.name.lower()} <= {self.threshold} ):'\n",
    "        \n",
    "        return check_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "302cfa27-e0e7-4f59-8e09-d69a4c735287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_split(data, decisionStump):\n",
    "    '''\n",
    "    Splits incoming data based on the provided decision stump.\n",
    "    Returns the left and right partitions resultant from the decision stump partitioning. \n",
    "    \n",
    "    Left partition is all the false cases.\n",
    "    Right partition is all the true cases.\n",
    "    '''\n",
    "    left, right = [],[]  \n",
    "    # Left is the false partition from the decision stump.\n",
    "    # Right is the true partition from the decision stump.\n",
    "    \n",
    "    for datapoint in data:\n",
    "        if decisionStump.eval(datapoint):\n",
    "            left.append(datapoint)\n",
    "        else:\n",
    "            right.append(datapoint)\n",
    "            \n",
    "    return left,right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b5ccfb95-57ff-43d6-b563-6ce5cd638941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(data):\n",
    "    '''\n",
    "    Counts the number of Assams and Bhutans in a given set of datapoints.\n",
    "    \n",
    "    Assumes the last column is the classification, and -1 --> Assam, +1 --> Bhutan\n",
    "    '''\n",
    "    assams, bhutans = 0,0\n",
    "    for datapoint in data:\n",
    "        if datapoint[-1] == -1:\n",
    "            assams = assams+1\n",
    "        else:\n",
    "            bhutans = bhutans + 1          \n",
    "    return assams,bhutans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b6ec3490-1409-45cc-8305-7bb2e7dcabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    '''\n",
    "    Calculates the entropy of the set of datapoints\n",
    "    '''\n",
    "    \n",
    "    # A very small value, added onto the log base 2 calculation to avoid a division by zero warning.\n",
    "    #\n",
    "    # > ( I conjecture I was running to this warning due to the right/total or left/total values being\n",
    "    #     very small. Adding this would not greatly affect the decision tree calculations, since a \n",
    "    #     log2 of a small value us largely negative, which would make the mixed entropy very large. This would\n",
    "    #     be promptly ignored, since we're trying to find argmin(mixed_entropy). \n",
    "    #   )\n",
    "\n",
    "    epsilon = 0.0000000001 \n",
    "    \n",
    "    # Left values are Assams, and right values are Bhuttans. \n",
    "    left, right = count_labels(data)\n",
    "    total = left + right\n",
    "    \n",
    "    if total == 0:\n",
    "        return None\n",
    "    \n",
    "    # Return the calculated entropy. \n",
    "    return -( left/total * np.log2( left/total + epsilon ) + right/total * np.log2( right/total + epsilon ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7c902938-355e-4b3d-ace1-40c4d7f73505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_decision_stump(training_data, attribute):\n",
    "    '''\n",
    "    For a given attribute, find the best value for a threshold, which minimizes the mixed entropy. \n",
    "    With that threshold value, construct and return a Decision Stump. \n",
    "    ''' \n",
    "    best_mixed_entropy = np.Infinity # Sentinel value, so that we can only go lower.\n",
    "    best_decision_stump = None # Sentinel value. Only returned if something goes wrong. \n",
    "    quantized_unit = attribute.quantized_unit\n",
    "    \n",
    "    # Iterate through all possible threshold values.\n",
    "    for threshold in attribute.values:\n",
    "        decision_stump = DecisionStump( attribute, threshold )     \n",
    "        \n",
    "        # Split the data into binary partitions. \n",
    "        left, right = binary_split(training_data, decision_stump)\n",
    "        n_total = len(training_data)\n",
    "        n_left = len(left)\n",
    "        n_right = len(right)\n",
    "    \n",
    "        # If the threshold does not split the data at all, we ignore this threshold value.\n",
    "        if ( n_left == 0 or n_right == 0 ):\n",
    "            continue\n",
    "    \n",
    "        # Calculate the mixed entropy. \n",
    "        mixed_entropy = n_left/n_total * entropy(left) + n_right/n_total * entropy(right)\n",
    "        \n",
    "        # Break ties by using the first value found.\n",
    "        # REASONING:\n",
    "        # This is because it will make comparing each decision stump against each other easier,\n",
    "        # since each threshold corresponding to an attribute will hold the mixed entropy, \n",
    "        # it wouldn't help in comparison, and the extra comparisons can be wasteful. \n",
    "    \n",
    "        if(mixed_entropy < best_mixed_entropy):\n",
    "            best_mixed_entropy = mixed_entropy\n",
    "            best_decision_stump = decision_stump\n",
    "    \n",
    "    return best_decision_stump, best_mixed_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d3920fe8-8574-417e-ae5a-6d0b13e04401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(training_data, attributes):\n",
    "    '''\n",
    "    Finds the decision stump which yields the best mixed entropy after splitting. \n",
    "    '''\n",
    "    \n",
    "    best_mixed_entropy = np.Infinity # Sentinel value, so that we can only go lower. \n",
    "    best_decision_stump = None # Sentinel value. Only returned if something goes wrong. \n",
    "    \n",
    "    # Iterate through all available attributes, and find the best decision stump. \n",
    "    for attribute in attributes:\n",
    "        decision_stump, mixed_entropy = find_optimal_decision_stump(training_data, attribute)\n",
    "        if(mixed_entropy < best_mixed_entropy):\n",
    "            best_mixed_entropy = mixed_entropy\n",
    "            best_decision_stump = decision_stump\n",
    "    \n",
    "    return best_decision_stump, best_mixed_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847418f-8fb3-4c16-8155-9c1226b3c8a0",
   "metadata": {},
   "source": [
    "#### Decision Tree Node Classes:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8cb04c3b-aaed-417f-a464-eb3e37411fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    '''\n",
    "    The Leaf Node classifies data.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        The prediction works according to the mode value of all the set of classifications \n",
    "        of the datapoints within the leaf node. i.e. It is the popular value of the classficiation.\n",
    "        '''\n",
    "        return mode(np.array(self.data)[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "cf6cf87c-d0ec-45f6-a6a8-96ae5cff38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    '''\n",
    "    A decision node splits incoming data into binary partitions based on a one-rule.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, decision_stump, left_child, right_child):\n",
    "        self.decision_stump = decision_stump\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2edb915d-7c42-4923-8fce-e51939db950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree(data, attributes, DEPTH=1):    \n",
    "    '''\n",
    "    This function recursively builds a decision tree from labeled data.\n",
    "    \n",
    "    Stopping conditions include:\n",
    "        --> The class consists of more than 95% of one class or the other.\n",
    "        --> Tree depth exceeds 10.\n",
    "        --> There are less than 9 datapoints within a node. \n",
    "    '''\n",
    "    global MIN_LEAF_SIZE\n",
    "    global MAX_DEPTH\n",
    "    global PERCENT_REPRESENTATION\n",
    "    \n",
    "    decision_stump, mixed_entropy = find_best_split(data, attributes)\n",
    "    assams, bhuttans = count_labels(data)\n",
    "    assams = assams/len(data)\n",
    "    bhuttans = bhuttans/len(data)\n",
    "    \n",
    "    # Check for stopping conditions. If conditions are met, return a leaf node. \n",
    "    if ( assams > PERCENT_REPRESENTATION/100 or bhuttans > PERCENT_REPRESENTATION/100 ) or ( DEPTH > MAX_DEPTH ) or ( len(data) < MIN_LEAF_SIZE ):\n",
    "        return LeafNode(data)\n",
    "    \n",
    "    # Recursively build the decision tree...\n",
    "    left, right = binary_split(data, decision_stump)    \n",
    "    return DecisionNode( decision_stump, generate_decision_tree(left, attributes, DEPTH+1), generate_decision_tree(right, attributes, DEPTH+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfb59d-2bab-4d2c-8f70-4475cd609e30",
   "metadata": {},
   "source": [
    "### Metaprogramming:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d1511067-862e-40e7-b63a-76a8d3062fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_header(attributes):\n",
    "    '''\n",
    "    Make sure the classifier program runs. \n",
    "    \n",
    "    Add all required imports, load the data into memory, quantize data, etc.\n",
    "    '''\n",
    "    global VALIDATION_SET\n",
    "    \n",
    "    print('import numpy as np','import pandas as pd', sep='\\n')\n",
    "    print()\n",
    "    \n",
    "    print(f\"FILENAME = '{VALIDATION_SET}'\")\n",
    "    print('csv = pd.read_csv(FILENAME)')\n",
    "    print()\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        print(f\"csv['{attribute.name}'] = csv['{attribute.name}'].apply( lambda datapoint: {attribute.quantized_unit} * round(datapoint/{attribute.quantized_unit}) )\")\n",
    "    print()\n",
    "    \n",
    "    print('predictions = []')\n",
    "    print()\n",
    "    \n",
    "    print('for datapoint in csv.to_numpy():')\n",
    "    print()\n",
    "    \n",
    "    print('    # Datapoint values:')\n",
    "    for attribute in attributes:\n",
    "        print(f'    {attribute.name.lower()} = datapoint[{attribute.column}]')\n",
    "    \n",
    "    print()\n",
    "    print('    prediction = -1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a1c1799b-6822-422b-a0b0-749792a288d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_body(node, padding=\"\"):\n",
    "    '''\n",
    "    Adds the decision tree if/else ladder to the classifier program.\n",
    "    '''\n",
    "    \n",
    "    if isinstance(node, LeafNode):        \n",
    "        print (padding + \"prediction =\", node.predict() )\n",
    "        return\n",
    "\n",
    "    print (padding + str(node.decision_stump))\n",
    "    emit_body(node.left_child, padding + \"    \")\n",
    "    print (padding + 'else:')\n",
    "    emit_body(node.right_child, padding + \"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a69e22c8-dc7b-45df-823d-6c2e92df69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_footer():\n",
    "    '''\n",
    "    Wraps up the classifier program. Makes sure results are output to a CSV file for earier viewing.\n",
    "    '''\n",
    "    \n",
    "    print()\n",
    "    print('    predictions.append(prediction)')\n",
    "    print('    print(prediction)')\n",
    "    print()\n",
    "    \n",
    "    print(\"df = pd.DataFrame(predictions,columns=['ClassID'])\")\n",
    "    print(\"df.to_csv('HW05_Mehboob_Mohammed_MyClassifications.csv',index=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e059ec-5810-46fd-9616-d6a90813b9e4",
   "metadata": {},
   "source": [
    "### Runner:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f9b54a0a-445c-4e74-83d5-e82a2e5fd2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Attribute[0]: Age / 2,\n",
       " Attribute[1]: Ht / 4,\n",
       " Attribute[2]: TailLn / 2,\n",
       " Attribute[3]: HairLn / 2,\n",
       " Attribute[4]: BangLn / 2,\n",
       " Attribute[5]: Reach / 2,\n",
       " Attribute[6]: EarLobes / 1]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = [ Attribute(name, column_id) for column_id, name in enumerate(csv.drop( columns = csv.columns[-2:] ).columns) ]; attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0a0ebf38-3d6f-49ac-a36c-39309c73f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = round_data(csv,attributes).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8daadb01-8016-4db2-82de-8ebb54ea3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = generate_decision_tree(data,attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "effff504-3399-41bb-81a9-3721570ffcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout_backup = sys.stdout\n",
    "with open('HW05_Mehboob_Mohammed_Trained_Classifier.py', 'w') as file:\n",
    "    sys.stdout = file\n",
    "    emit_header(attributes)\n",
    "    emit_body(decision_tree, '    ')\n",
    "    emit_footer()\n",
    "    sys.stdout = stdout_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16762ff0-8c88-4cab-bd44-88ccc3705a1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e36d9be7-2421-42f3-8704-abda514dc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9526\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive (A)</th>\n",
       "      <th>Negative (A)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2360</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>2403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive (A)  Negative (A)\n",
       "0          2360            97\n",
       "1           140          2403"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate confusion matrix for test data:\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "my_classifications = pd.read_csv('HW05_Mehboob_Mohammed_MyClassifications.csv').to_numpy()  # Set VALIDATION_SET = TEST_SET before running mentor.\n",
    "for index, classification in enumerate(my_classifications):\n",
    "    if classification == -1:\n",
    "        if csv['ClassID'][index] == -1:\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "    else:\n",
    "        if csv['ClassID'][index] == 1:\n",
    "            TN = TN + 1\n",
    "        else:\n",
    "            FN = FN + 1\n",
    "        \n",
    "print('Accuracy:', (TP+TN)/(TP+TN+FP+FN))\n",
    "pd.DataFrame([ [TP,FP], [FN, TN] ], columns = ['Positive (A)', 'Negative (A)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82427e48-eec7-40f7-a5d3-8d0d98b0b10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
